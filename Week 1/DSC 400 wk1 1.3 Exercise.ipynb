{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **1.3 Exercise**\n",
    "Michael J. Montana\n",
    "College of Science and Tecnology, Bellevue University\n",
    "DSC400: Big Data, Technology, and Algorithms\n",
    "Professor Shawn Hermans\n",
    "June 11 2023"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Creating a Big Data Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Assignment 2\n",
    "\n",
    "Big data is a rapidly evolving, ever-changing field. Keeping track of the latest big data stacks, programming libraries, software, and other tools requires constant vigilance. Any book on big data will be out of date by the time it is published. We need a resource that is updated on a more frequent basis. \n",
    "\n",
    "This assignment will help create that resource by researching the latest big data tools and technologies. We will use this research to create an *Awesome Big Data* list. Below is a list of similar *awesome* lists that may be useful when creating our *Awesome Big Data* list. \n",
    "\n",
    "*[Awesome Python](https://awesome-python.com/)* is a curated list of awesome Python frameworks, libraries, software and resources. It was inspired by [awesome-php](https://github.com/ziadoz/awesome-php). \n",
    "\n",
    "*[Awesome Jupyter](https://github.com/markusschanta/awesome-jupyter)* is a curated list of awesome Jupyter projects, libraries and resources. Jupyter is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text.\n",
    "\n",
    "*[Awesome Dash](https://github.com/ucg8j/awesome-dash)* is a curated list of awesome Dash (plotly) resources. Dash is a productive Python framework for building web applications. Written on top of Flask, Plotly.js, and React.js, Dash is ideal for building data visualization apps with highly custom user interfaces in pure Python. It's particularly suited for anyone who works with data in Python.\n",
    "\n",
    "*[Awesome JavaScript](https://github.com/sorrycc/awesome-javascript)* is a collection of awesome browser-side JavaScript libraries, resources and shiny things. The [data visualization section](https://github.com/sorrycc/awesome-javascript#data-visualization) may be of use. \n",
    "\n",
    "*[Awesome Deep Learning](https://github.com/ChristosChristofidis/awesome-deep-learning)* is a curated list of awesome Deep Learning tutorials, projects and communities.\n",
    "\n",
    "*[Awesome Machine Learning](https://github.com/josephmisiti/awesome-machine-learning)* is a curated list of awesome machine learning frameworks, libraries and software (by language).\n",
    "\n",
    "*[Awesome Data Engineering](https://github.com/igorbarinov/awesome-data-engineering)* is a curated list of data engineering tools for software developers. \n",
    "\n",
    "*[Awesome Public Datasets](https://github.com/awesomedata/awesome-public-datasets)* is a list of a topic-centric public data sources in high quality. They are collected and tidied from blogs, answers, and user responses. \n",
    "\n",
    "*[Awesome](https://github.com/sindresorhus/awesome)* is a list of awesome lists about all kinds of interesting topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assignment 2.1\n",
    "\n",
    "Before we get started, we will access your knowledge of big data by taking the [Pokémon or Big Data Quiz](http://pixelastic.github.io/pokemonorbigdata/). Don't worry. The quiz results won't impact your grade. \n",
    "\n",
    "Included below is code that fetches the answers to the questions and provides the results in a Pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "quiz_answers_json = 'https://raw.githubusercontent.com/pixelastic/pokemonorbigdata/master/app/questions.json'\n",
    "df_all = pd.read_json(quiz_answers_json)\n",
    "# Pokémon answers\n",
    "df_all[df_all['type'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Big data answers\n",
    "df_all[df_all['type'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assignment 2.2\n",
    "\n",
    "In the next part of the assignment, you will populate the items with categories for our list. The first chapter of the textbook, *Big Data Science & Analytics: A Hands-On Approach*, provides list of categories and subcategories for the big data stack. We will use these categories as a starting point, but will not constrain ourselves to them. \n",
    "\n",
    "When creating categories, avoid deeply nested layers of categories and subcategories. At most, define a top-level category with multiple subcategories. We will start with the following high-level categories and subcategories. \n",
    "\n",
    "***Categories***\n",
    "\n",
    "We will use the disutils trove classification convention defined in [PEP 301](https://www.python.org/dev/peps/pep-0301/) when defining a category with a subcategory. This convention uses the double-colon (\"::\") to separate a category and subcategory. The following is an example of categories and subcategories as defined in the first chapter of the textbook, *Big Data Science & Analytics: A Hands-On Approach*. \n",
    "\n",
    "- Batch Analysis :: DAG\n",
    "- Batch Analysis :: Machine Learning\n",
    "- Batch Analysis :: MapReduce\n",
    "- Batch Analysis :: Script\n",
    "- Batch Analysis :: Search\n",
    "- Batch Analysis :: Workflow Scheduling\n",
    "- Data Access Connector :: Custom Connectors\n",
    "- Data Access Connector :: Publish-Subscribe\n",
    "- Data Access Connector :: Queues\n",
    "- Data Access Connector :: SQL\n",
    "- Data Access Connector :: Source-Sink\n",
    "- Data Storage :: Distributed File System\n",
    "- Data Storage :: NoSQL\n",
    "- Deployment :: NoSQL\n",
    "- Deployment :: SQL\n",
    "- Deployment :: Visualization Frameworks\n",
    "- Deployment :: Web Frameworks\n",
    "- Interactive Querying :: Analytic SQL\n",
    "- Real-Time Analysis :: In-Memory\n",
    "- Real-Time Analysis :: Stream Processing\n",
    "\n",
    "Below is a list containing categories and suggested starting points for research. Fill in a least two items from each of the suggested categories. Create at least one category that is not listed and add two items to that category. \n",
    "\n",
    "* AI and Machine Learning\n",
    "    * Apache Spark's MLlib\n",
    "    * H2O\n",
    "    * Tensorflow\n",
    "* Batch Processing\n",
    "    * Apache\n",
    "    * Apache Spark\n",
    "    * Dask\n",
    "    * MapReduce\n",
    "* Cloud and Data Platforms\n",
    "    * Amazon Web Services\n",
    "    * Cloudera Data Platform\n",
    "    * Google Cloud Platform\n",
    "    * Microsoft Azure\n",
    "* Container Engines and Orchestration\n",
    "    * Docker\n",
    "    * Docker Swarm\n",
    "    * Kubernetes\n",
    "    * Podman\n",
    "* Data Storage :: Block Storage\n",
    "    * Amazon EBS\n",
    "    * OpenEBS\n",
    "* Data Storage :: Cluster Storage\n",
    "    * Ceph\n",
    "    * HDFS\n",
    "* Data Storage :: Object Storage\n",
    "    * Amazon S3\n",
    "    * Minio\n",
    "* Data Transfer Tools\n",
    "    * Apache Sqoop\n",
    "* Full-Text Search\n",
    "    * Apache Solr\n",
    "    * Elasticsearch\n",
    "* Interactive Query\n",
    "    * Apache Hive\n",
    "    * Google Big Query\n",
    "    * Spark SQL\n",
    "* Message Queues\n",
    "    * Apache Kafka\n",
    "    * RabbitMQ\n",
    "* NoSQL :: Document Databases\n",
    "    * CouchDB\n",
    "    * Google Firestore\n",
    "    * MongoDB\n",
    "* NoSQL :: Graph Databases\n",
    "    * DGraph\n",
    "    * Neo4j\n",
    "* NoSQL :: Key-Value Databases\n",
    "    * Amazon DynamoDB\n",
    "* NoSQL :: Time-Series Databases\n",
    "    * TSDB\n",
    "* Serverless Functions\n",
    "    * AWS Lambda\n",
    "    * OpenFaaS\n",
    "* Stream Processing\n",
    "    * Apache Spark's Structured Streaming\n",
    "    * Apache Storm\n",
    "    * Google Dataflow\n",
    "* Visualization Frameworks\n",
    "    * Apache Superset\n",
    "    * Redash\n",
    "* Workflow Engine\n",
    "    * Apache Airflow\n",
    "    * Google Cloud Composer\n",
    "    * Oozie\n",
    "    \n",
    "We populate the list items using the `ListItem` class, defined below. The following is a description of the `ListItem` fields. \n",
    "\n",
    "**name**\n",
    "\n",
    "The proper name of the list item\n",
    "\n",
    "**website**\n",
    "\n",
    "Link to the item's website.  Include `http://` or `https://` in the link. \n",
    "\n",
    "**category**\n",
    "\n",
    "Category and optional subcategory for the item. \n",
    "\n",
    "**short_description**\n",
    "\n",
    "Provide a short, one to two-sentence description of the item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ListItem:\n",
    "    name: str\n",
    "    website: str\n",
    "    category: str\n",
    "    short_description: str\n",
    "    \n",
    "all_items = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following is an example of creating the entry for AWS as a seperate variable and then adding it to the `all_items` set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "aws = ListItem(\n",
    "    'Amazon Web Services',\n",
    "    'https://aws.amazon.com/',\n",
    "    'Cloud and Data Platforms',\n",
    "    \"\"\"Provides on-demand cloud computing platforms and APIs to individuals, \n",
    "    companies, and governments, on a metered pay-as-you-go basis.\"\"\"\n",
    ")\n",
    "\n",
    "all_items.add(aws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can also add an item to the list directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_items.remove(aws)\n",
    "all_items.add(ListItem(\n",
    "    'Amazon Web Services',\n",
    "    'https://aws.amazon.com/',\n",
    "    'Cloud and Data Platforms',\n",
    "    \"\"\"Provides on-demand cloud computing platforms and APIs to individuals, \n",
    "    companies, and governments, on a metered pay-as-you-go basis.\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Fill in a least two items from each of the suggested categories.\n",
    "\n",
    "all_items= [\n",
    "    # AI and Machine Learning\n",
    "    ListItem(\n",
    "        \"Apache Spark's MLlib\",\n",
    "        \"https://spark.apache.org/mllib/\",\n",
    "        \"AI and Machine Learning\",\n",
    "        \"\"\"MLlib is Apache Spark's scalable machine learning library.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"H2O\",\n",
    "        \"https://h2o.ai/\",\n",
    "        \"AI and Machine Learning\",\n",
    "        \"\"\"Powered by world-class automated machine learning, the H2O AI Cloud enables organizations to build predictive models and gain insights from their data quickly and easily.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"Tensorflow\",\n",
    "        \"https://www.tensorflow.org/\",\n",
    "        \"AI and Machine Learning\",\n",
    "        \"\"\"TensorFlow is an end-to-end open source platform for machine learning. \"\"\"\n",
    "    ),\n",
    "    # Batch Processing\n",
    "    ListItem(\n",
    "        \"Apache Spark\",\n",
    "        \"https://spark.apache.org/\",\n",
    "        \"Batch Processing\",\n",
    "        \"\"\"Apache Spark™ is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"Dask\",\n",
    "        \"https://www.dask.org/\",\n",
    "        \"Batch Processing\",\n",
    "        \"\"\"Dask makes it easy to scale the Python libraries that you know and love like NumPy, pandas, and scikit-learn.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"MapReduce\",\n",
    "        \"https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html\",\n",
    "        \"Batch Processing\",\n",
    "        \"\"\"Hadoop MapReduce is a software framework for easily writing applications which process vast amounts of data (multi-terabyte data-sets) in-parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner.\"\"\"\n",
    "    ),\n",
    "    # Cloud and Data Platforms\n",
    "    ListItem(\n",
    "        \"Amazon Web Services\",\n",
    "        \"https://aws.amazon.com/\",\n",
    "        \"Cloud and Data Platforms\",\n",
    "        \"\"\"Provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered pay-as-you-go basis.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "    \"Cloudera Data Platform\",\n",
    "        \"https://www.cloudera.com/\",\n",
    "        \"Cloud and Data Platforms\",\n",
    "        \"\"\"Cloudera Data Platform (CDP) is a hybrid data platform designed for unmatched freedom to choose—any cloud, any analytics, any data.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"Google Cloud Platform\",\n",
    "        \"https://cloud.google.com/\",\n",
    "        \"Cloud and Data Platforms\",\n",
    "        \"\"\"A transformation cloud accelerates an organization’s digital transformation through data democratization, app and infrastructure modernization, people connections, and trusted transactions.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"Microsoft Azure\",\n",
    "        \"https://azure.microsoft.com/en-us\",\n",
    "        \"Cloud and Data Platforms\",\n",
    "        \"\"\"On-premises, hybrid, multicloud, or at the edge—create secure, future-ready cloud solutions on Azure\"\"\"\n",
    "    ),\n",
    "    # Container Engines and Orchestration\n",
    "    ListItem(\n",
    "        \"Docker\",\n",
    "        \"https://www.docker.com/\",\n",
    "        \"Container Engines and Orchestration\",\n",
    "        \"\"\"Docker Engine is an open source containerization technology for building and containerizing your applications.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"Docker Swarm\",\n",
    "        \"https://docs.docker.com/engine/swarm/\",\n",
    "        \"Container Engines and Orchestration\",\n",
    "        \"\"\"Swarm mode is an advanced feature for managing a cluster of Docker daemons.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"Kubernetes\",\n",
    "        \"https://kubernetes.io/\",\n",
    "        \"Container Engines and Orchestration\",\n",
    "        \"\"\"Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"Podman\",\n",
    "        \"https://podman.io/\",\n",
    "        \"Container Engines and Orchestration\",\n",
    "        \"\"\"Manage containers, pods, and images with Podman. Seamlessly work with containers and Kubernetes from your local environment.\"\"\"\n",
    "    ),\n",
    "    # Data Storage :: Block Storage\n",
    "    ListItem(\n",
    "        \"Amazon EBS\",\n",
    "        \"https://aws.amazon.com/ebs/\",\n",
    "        \"Data Storage :: Block Storage\",\n",
    "        \"\"\"Amazon Elastic Block Store (Amazon EBS) is an easy-to-use, scalable, high-performance block-storage service designed for Amazon Elastic Compute Cloud (Amazon EC2).\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"OpenEBS\",\n",
    "        \"https://openebs.io/\",\n",
    "        \"Data Storage :: Block Storage\",\n",
    "        \"\"\"OpenEBS helps Developers and Platform SREs easily deploy Kubernetes Stateful Workloads that require fast and highly reliable container attached storage. OpenEBS turns any storage available on the Kubernetes worker nodes into local or distributed Kubernetes Persistent Volumes.\"\"\"\n",
    "    ),\n",
    "    # Data Storage :: Cluster Storage\n",
    "    ListItem(\n",
    "        \"Ceph\",\n",
    "        \"https://ceph.io/en/\",\n",
    "        \"Data Storage :: Cluster Storage\",\n",
    "        \"\"\"Use Ceph to transform your storage infrastructure. Ceph provides a unified storage service with object, block, and file interfaces from a single cluster built from commodity hardware components.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"HDFS\",\n",
    "        \"https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html\",\n",
    "        \"Data Storage :: Cluster Storage\",\n",
    "        \"\"\"The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. HDFS relaxes a few POSIX requirements to enable streaming access to file system data. HDFS was originally built as infrastructure for the Apache Nutch web search engine project. HDFS is part of the Apache Hadoop Core project.\"\"\"\n",
    "    ),\n",
    "    # Data Storage :: Object Storage\n",
    "    ListItem(\n",
    "        \"Amazon S3\",\n",
    "        \"https://aws.amazon.com/s3/\",\n",
    "        \"Data Storage :: Object Storage\",\n",
    "        \"\"\"Amazon Simple Storage Service (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can store and protect any amount of data for virtually any use case, such as data lakes, cloud-native applications, and mobile apps. With cost-effective storage classes and easy-to-use management features, you can optimize costs, organize data, and configure fine-tuned access controls to meet specific business, organizational, and compliance requirements.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"Minio\",\n",
    "        \"https://min.io/\",\n",
    "        \"Data Storage :: Object Storage\",\n",
    "        \"\"\"MinIO is a high-performance, S3 compatible object store. It is built for large scale AI/ML, data lake and database workloads. It runs on-prem and on any cloud (public or private) and from the data center to the edge. MinIO is software-defined and open source under GNU AGPL v3.\"\"\"\n",
    "    ),\n",
    "    # Data Transfer Tools\n",
    "    ListItem(\n",
    "        \"Apache Sqoop\",\n",
    "        \"https://sqoop.apache.org/\",\n",
    "        \"Data Transfer Tools\",\n",
    "        \"\"\"This project has retired.  Apache Sqoop(TM) is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases.\"\"\"\n",
    "    ),\n",
    "    # Full-Text Search\n",
    "    ListItem(\n",
    "        \"Apache Solr\",\n",
    "        \"https://solr.apache.org/\",\n",
    "        \"Full-Text Search\",\n",
    "        \"\"\"Solr is highly reliable, scalable and fault tolerant, providing distributed indexing, replication and load-balanced querying, automated failover and recovery, centralized configuration and more. Solr powers the search and navigation features of many of the world's largest internet sites.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"Elasticsearch\",\n",
    "        \"https://www.elastic.co/\",\n",
    "        \"Full-Text Search\",\n",
    "        \"\"\"Powered by advanced machine learning, Elastic Observability is an open and flexible solution that accelerates problem resolution, provides end-to-end visibility into hybrid and multi-cloud environments, and unifies logs, metrics, and traces.\"\"\"\n",
    "    ),\n",
    "    # Interactive Query\n",
    "    ListItem(\n",
    "        \"Apache Hive\",\n",
    "        \"https://hive.apache.org/\",\n",
    "        \"Interactive Query\",\n",
    "        \"\"\"The Apache Hive ™ is a distributed, fault-tolerant data warehouse system that enables analytics at a massive scale and facilitates reading, writing, and managing petabytes of data residing in distributed storage using SQL.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"Google Big Query\",\n",
    "        \"https://cloud.google.com/bigquery/\",\n",
    "        \"Interactive Query\",\n",
    "        \"\"\"BigQuery is a serverless and cost-effective enterprise data warehouse that works across clouds and scales with your data. Use built-in ML/AI and BI for insights at scale.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"Spark SQL\",\n",
    "        \"https://spark.apache.org/sql/\",\n",
    "        \"Interactive Query\",\n",
    "        \"\"\"Spark SQL is Apache Spark's module for working with structured data. Spark SQL lets you query structured data inside Spark programs, using either SQL or a familiar DataFrame API. Usable in Java, Scala, Python and R.\"\"\"\n",
    "    ),\n",
    "    # Message Queues\n",
    "    ListItem(\n",
    "        \"Apache Kafka\",\n",
    "        \"https://kafka.apache.org/\",\n",
    "        \"Message Queues\",\n",
    "        \"\"\"Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"RabbitMQ\",\n",
    "        \"https://www.rabbitmq.com/\",\n",
    "        \"Message Queues\",\n",
    "        \"\"\"RabbitMQ is an open source message broker.  RabbitMQ is lightweight and easy to deploy on premises and in the cloud. It supports multiple messaging protocols. RabbitMQ can be deployed in distributed and federated configurations to meet high-scale, high-availability requirements.\"\"\"\n",
    "    ),\n",
    "    # NoSQL :: Document Databases\n",
    "    ListItem(\n",
    "        \"CouchDB\",\n",
    "        \"https://couchdb.apache.org/\",\n",
    "        \"Document Databases\",\n",
    "        \"\"\"Seamless multi-master sync, that scales from Big Data to Mobile, with an Intuitive HTTP/JSON API and designed for Reliability.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"Google Firestore\",\n",
    "        \"https://firebase.google.com/docs/firestore\",\n",
    "        \"Document Databases\",\n",
    "        \"\"\"Cloud Firestore is a flexible, scalable database for mobile, web, and server development from Firebase and Google Cloud.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"MongoDB\",\n",
    "        \"https://www.mongodb.com/\",\n",
    "        \"Document Databases\",\n",
    "        \"\"\"MongoDB Atlas combines the leading document-oriented database with a full suite of developer tools for accelerating app development while eliminating integration work.\"\"\"\n",
    "    ),\n",
    "    # NoSQL :: Graph Databases\n",
    "    ListItem(\n",
    "        \"DGraph\",\n",
    "        \"https://dgraph.io/\",\n",
    "        \"NoSQL :: Graph Databases\",\n",
    "        \"\"\"Graph database with native GraphQL and graph query language\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"Neo4j\" ,\n",
    "        \"https://neo4j.com/\",\n",
    "        \"NoSQL :: Graph Databases\",\n",
    "        \"\"\"The world's leading graph database as a fully-managed cloud service — zero-admin, globally available and always-on.\"\"\"\n",
    "    ),\n",
    "    # NoSQL :: Key-Value Databases\n",
    "    ListItem(\n",
    "        \"Amazon DynamoDB\",\n",
    "        \"https://aws.amazon.com/dynamodb/\",\n",
    "        \"NoSQL :: Key-Value Databases\",\n",
    "        \"\"\"Amazon DynamoDB is a fully managed, serverless, key-value NoSQL database designed to run high-performance applications at any scale. DynamoDB offers built-in security, continuous backups, automated multi-Region replication, in-memory caching, and data import and export tools.\"\"\"\n",
    "    ),\n",
    "    # NoSQL :: Time-Series Databases\n",
    "    ListItem(\n",
    "        \"OpenTSDB\",\n",
    "        \"http://opentsdb.net/overview.html\",\n",
    "        \"NoSQL :: Time-Series Databases\",\n",
    "        \"\"\"OpenTSDB consists of a Time Series Daemon (TSD) as well as set of command line utilities. Interaction with OpenTSDB is primarily achieved by running one or more of the TSDs.\"\"\"\n",
    "    ),\n",
    "    # Serverless Functions\n",
    "    ListItem(\n",
    "        \"AWS Lambda\",\n",
    "        \"https://aws.amazon.com/lambda/\",\n",
    "        \"Serverless Functions\",\n",
    "        \"\"\"AWS Lambda is a serverless, event-driven compute service that lets you run code for virtually any type of application or backend service without provisioning or managing servers. You can trigger Lambda from over 200 AWS services and software as a service (SaaS) applications, and only pay for what you use.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"OpenFaaS\",\n",
    "        \"https://www.openfaas.com/\",\n",
    "        \"Serverless Functions\",\n",
    "        \"\"\"OpenFaaS® makes it easy for developers to deploy event-driven functions and microservices to Kubernetes without repetitive, boiler-plate coding. Package your code or an existing binary in a Docker image to get a highly scalable endpoint with auto-scaling and metrics.\"\"\"\n",
    "    ),\n",
    "    # Stream Processing\n",
    "    ListItem(\n",
    "        \"Apache Spark's Structured Streaming\",\n",
    "        \"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#overview\",\n",
    "        \"Stream Processing\",\n",
    "        \"\"\"Spark Structured Streaming is a stream processing engine built on Spark SQL that processes data incrementally and updates the final results as more streaming data arrives. It brought a lot of ideas from other structured APIs in Spark (Dataframe and Dataset) and offered query optimizations similar to SparkSQL.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"Apache Storm\",\n",
    "        \"https://storm.apache.org/\",\n",
    "        \"Stream Processing\",\n",
    "        \"\"\"Apache Storm is a free and open source distributed realtime computation system. Apache Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Apache Storm is simple, can be used with any programming language, and is a lot of fun to use!\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"Google Dataflow\",\n",
    "        \"https://cloud.google.com/dataflow\",\n",
    "        \"Stream Processing\",\n",
    "        \"\"\"Unified stream and batch data processing that's serverless, fast, and cost-effective.  Fully managed data processing service.  Automated provisioning and management of processing resources.\"\"\"\n",
    "    ),\n",
    "    # Visualization Frameworks\n",
    "    ListItem(\n",
    "        \"Apache Superset\",\n",
    "        \"https://superset.apache.org/\",\n",
    "        \"Visualization Frameworks\",\n",
    "        \"\"\"Apache Superset is a modern data exploration and visualization platform. Superset is fast, lightweight, intuitive, and loaded with options that make it easy for users of all skill sets to explore and visualize their data, from simple line charts to highly detailed geospatial charts.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"Redash\",\n",
    "        \"https://redash.io/\",\n",
    "        \"Visualization Frameworks\",\n",
    "        \"\"\"Redash helps you make sense of your data.  Connect and query your data sources, build dashboards to visualize data and share them with your company.\"\"\"\n",
    "    ),\n",
    "    # Workflow Engine\n",
    "    ListItem(\n",
    "        \"Apache Airflow\",\n",
    "        \"https://airflow.apache.org/\",\n",
    "        \"Workflow Engine\",\n",
    "        \"\"\"Airflow is a platform created by the community to programmatically author, schedule and monitor workflows.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"Google Cloud Composer\",\n",
    "        \"https://cloud.google.com/composer\",\n",
    "        \"Workflow Engine\",\n",
    "        \"\"\"A fully managed workflow orchestration service built on Apache Airflow. Author, schedule, and monitor pipelines that span across hybrid and multi-cloud environments.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"Oozie\",\n",
    "        \"https://oozie.apache.org/\",\n",
    "        \"Workflow Engine\",\n",
    "        \"\"\"Oozie is a workflow scheduler system to manage Apache Hadoop jobs.  Oozie Workflow jobs are Directed Acyclical Graphs (DAGs) of actions.  Oozie Coordinator jobs are recurrent Oozie Workflow jobs triggered by time (frequency) and data availability.\"\"\"\n",
    "    ),\n",
    "\n",
    "# TODO: Create at least one category that is not listed and add two items to that category.\n",
    "\n",
    "    #Data Access Connector :: Publish-Subscribe\n",
    "    ListItem(\n",
    "        \"AWS Kinesis\",\n",
    "        \"https://aws.amazon.com/kinesis/\",\n",
    "        \"Data Access Connector :: Publish-Subscribe\",\n",
    "        \"\"\"Amazon Kinesis cost-effectively processes and analyzes streaming data at any scale as a fully managed service. With Kinesis, you can ingest real-time data, such as video, audio, application logs, website clickstreams, and IoT telemetry data, for machine learning (ML), analytics, and other applications.\"\"\"\n",
    "    ),\n",
    "    ListItem(\n",
    "        \"Azure Event Hubs\",\n",
    "        \"https://azure.microsoft.com/en-us/products/event-hubs\",\n",
    "        \"Data Access Connector :: Publish-Subscribe\",\n",
    "        \"\"\"Event Hubs is a fully managed, real-time data ingestion service that’s simple, trusted, and scalable. Stream millions of events per second from any source to build dynamic data pipelines and immediately respond to business challenges.\"\"\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assignment 2.3 (Optional)\n",
    "\n",
    "Use the `all_items` data to create Markdown output that mirrors the output of [Awesome Python](https://raw.githubusercontent.com/vinta/awesome-python/master/README.md). You can use the `jinja2` template engine to complete this task. This part of the assignment is entirely optional and is not graded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import jinja2\n",
    "\n",
    "template = jinja2.Template(\"\"\"\n",
    "# Awesome Big Data\n",
    "\n",
    "A curated list of awesome big data frameworks, libraries, software and resources.\n",
    "\n",
    "Inspired by [awesome-php](https://github.com/ziadoz/awesome-php).\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}